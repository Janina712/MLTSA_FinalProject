{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random forest classifier.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOpj6vXOxLDWcfxuwvRfaUG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Janina712/MLTSA_FinalProject/blob/main/tryouts/Random_forest_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pylab as pl\n",
        "from scipy import stats \n",
        "from scipy.stats import linregress\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import backend as K\n",
        "import glob\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from keras import layers \n",
        "from keras.layers import Dense, Input, Flatten,\\\n",
        "                   Reshape, LeakyReLU as LR,\\\n",
        "                   Activation, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "Rk14Y3f6g52r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v89krcohgrL1"
      },
      "outputs": [],
      "source": [
        "#all features df of size (number of windows, 16*21)\n",
        "#labels of size (number of windows, )\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features_combined, labels, \n",
        "                                                                            test_size = 0.25, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training Features Shape:', train_features.shape)\n",
        "print('Training Labels Shape:', train_labels.shape)\n",
        "print('Testing Features Shape:', test_features.shape)\n",
        "print('Testing Labels Shape:', test_labels.shape)"
      ],
      "metadata": {
        "id": "XnULVemohI0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model with 1000 decision trees\n",
        "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42, min_samples_leaf = 3)\n",
        "# Train the model on training data\n",
        "rf.fit(train_features, train_labels);"
      ],
      "metadata": {
        "id": "yeCGGoy9hLdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The baseline predictions are the historical averages\n",
        "baseline_preds = test_features[:, labels]\n",
        "# Baseline errors, and display average baseline error\n",
        "baseline_errors = abs(baseline_preds - test_labels)\n",
        "print('Average baseline error: ', round(np.mean(baseline_errors), 2))"
      ],
      "metadata": {
        "id": "GmOoJ7DJhOl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the forest's predict method on the test data\n",
        "predictions = rf.predict(test_features)\n",
        "# Calculate the absolute errors\n",
        "errors = abs(predictions - test_labels)\n",
        "# Print out the mean absolute error (mae)\n",
        "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
      ],
      "metadata": {
        "id": "SIXC5zWXhiYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean absolute percentage error (MAPE)\n",
        "mape = 100 * (errors / test_labels)\n",
        "# Calculate and display accuracy\n",
        "accuracy = 100 - np.mean(mape)\n",
        "print('Accuracy:', round(accuracy, 2), '%.')"
      ],
      "metadata": {
        "id": "65SjM4RThnQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_score = rf.score(train_features, train_labels)\n",
        "test_score = rf.score(test_features, test_labels)"
      ],
      "metadata": {
        "id": "xq9jcT7qhqU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('During training, the accuracy score was ' + str(round(train_score, 3)))\n",
        "print('During testing, the accuracy score was ' + str(round(test_score, 3)))"
      ],
      "metadata": {
        "id": "Ud5U6L1PhsMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do we need graphs for loss function here?"
      ],
      "metadata": {
        "id": "bh7-sgUVhuoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get numerical feature importances\n",
        "importances = list(rf.feature_importances_)\n",
        "# List of tuples with variable and importance\n",
        "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
        "# Sort the feature importances by most important first\n",
        "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
        "# Print out the feature and importances \n",
        "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
      ],
      "metadata": {
        "id": "l6TRIhkeh3Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl.figure()\n",
        "pl.plot(test_labels, \"o\", label = 'true')\n",
        "pl.plot(predictions, \"o\", label = 'prediction')\n",
        "pl.legend()\n",
        "pl.xlabel('Astronomical objects')\n",
        "pl.ylabel('Target')\n",
        "pl.title('');"
      ],
      "metadata": {
        "id": "YZelGMAWh6zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cf_matrix = confusion_matrix(test_labels, predictions)"
      ],
      "metadata": {
        "id": "fKgfb3K5h-pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl.figure(figsize=(12,8))\n",
        "ax = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues')\n",
        "\n",
        "ax.set_title('Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.set_xticklabels(meta.target.unique())\n",
        "ax.set_yticklabels(meta.target.unique())\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "pl.show()"
      ],
      "metadata": {
        "id": "pgVdfNzfiEBk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}